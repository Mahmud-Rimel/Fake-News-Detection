# Fake News Detection

The motivation of this project is to differentiate between Fake news and Real news by applying Natural Language processing techniques and machine learning algorithms.

## Workflow of the study
- **Removing Punctuation** 
- Tokenization
- Stopwords
- Stemming
- Feature Extraction
- Building machine learning classifiers
- Evaluation of the results

**Tokenization**

Tokenization is a technique to break the wordr of text. A text may contain with a huge number of words, sentences. Tokenization creates a list of words in the text.

**Stopwords**

Stopwords are words which has little value in a text/very common in a text.

**Stemming**

Stemming is a technique to reduce the abundant of a word. Stemming removes the suffixes of words and return a base form of the word

